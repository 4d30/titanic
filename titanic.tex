\documentclass[10pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{float}
\usepackage{enumitem}
\usepackage{indentfirst}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{multicol}

\graphicspath{ {./} }

%\setlength{\parindent}{0pt}

\title{Predicting Survival on the RMS Titanic\\
	\large{Binary Classification}}
\author{	Joseph Mifsud  \\
		Hungry for Opportunity}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}

	A study was performed to model passenger survival on the RMS Titanic.
	The model training dataset including 11 features for 891 passengers was downloaded from Kaggle.com.
	Missing values for specific features were modeled and estimated.
	Each feature was tested for significance and correlation with survival with Pearson's r metric.
	A logistic regressor was created from these signifcant features to estimate survival.
	The model was tested on a reserved sample of 491 passengers.
	The test yielded an accuracy of (0.87 $\pm$ 0.035) and an f1-score of (0.82 $\pm$ 0.040).
	Although variables were not analyzed for correlation with one another, this is a useful model for predicting the survival of passengers.

\end{abstract}
\pagebreak


\begin{multicols}{2}
\section{Introduction}
\subsection{Background}
	
	Binary classification is an important tool for every data scientist.
	Whether an object or action can be categorized into a feature of interest or not is inherently an task of binary classification.
	Binary classification could be use to help a farmer determine whether or not to plant a field or inform a physician if a growth is likely cancerous.
	This tool's use-case is very common.
	In demonstration of this machine learning technique, features of the passengers on the ill-fated RMS Titanic are used to estimate his or her survival.
	
\subsection{Problem}	
	
	"Which are the models and variables that produce the greatest f1-score for our test-set?"

\subsection{Interest}
	
	Binary classification is an incredibly useful tool for many different types of activities.
	Binary classification is of interest to the marketing department when answering the question "Is this person a potential customer?"
	It can answer the banker's question of "Should I provide this loan?"
	In medicine, binary classification can be useful to answer the question "Is this a benign growth?"
	Even the farmer who wants to know if a field should be planted can be assisted by binary classification.
	This versatile tool is of interest of a wide range of individuals in every industry.

\section{Data} \label{documentclasses}
\subsection{Data Sources}

	Kaggle.com provided the dataset for this analysis.
%	Features included in the dataset are:
%	\begin{itemize}[noitemsep]
%		\item PassengerId: A unique number associated with each passenger
%		\item Survived: Did this passenger survive?
%		\item Pclass: Passenger class
%		\item Name: Passenger's name
%		\item Sex: Male or Female
%		\item Age: Passenger's age
%		\item SibSp: Number of the passenger's siblings and spouses also on board
%		\item Parch: Number of the passenger's children and parents also on board
%		\item Ticket: The passeneger's ticket number
%		\item Fare: The cost of the passenger's ticket
%		\item Cabin: Which cabin the passenger stayed in
%		\item Embarked: From which port the passenger embarked.
%	\end{itemize}

\subsection{Data Cleaning}
	

	There were three features of the dataset which were missing values for a number of passengers: Age, Cabin, and Embarked.
	The most straitforward feature to address was which port the passenger embarked from.
	The passenger's fare and passenger class would be indicative of this or her port of embarkation.
	A decision-tree classifier was defined and fit to these features from 711 passengers.
	The decision-tree model was tested against a reserve sample of 178 passengers yielding an accuracy score of 0.90 and an f1-score of 0.90.
	The port of embarkation for the two target passengers was then estimated with this model.
	It was estimated that both of these passengers embarked at Southampton.\\
	\begin{center}
	\includegraphics[width=0.3725\textwidth]{embarked_dt}\\
	\end{center}
	177 passengers in this dataset did not have an age.
	There are various techniques to handle missing values in data.
	One such technique is imputation.
	A kNN model of the age of the passengers was created.
	An iterative approach was used to determine the number of neighbors in the passenger model.
	The number of neighbor was varied in a for-loop between 1 and 20 neighbors.
	The RMSE for each number of neighbors was calculated and averaged over 500 iterations.
	The average RMSE was plotted against the number of neighbors.
	\begin{center}	
	\includegraphics[width=0.45\textwidth]{age_rmse}\\
	\end{center}
	The "elbow method' was use to select the number of neighbors in our kNN model.
	Four neighbors were selected to estimate the age of each passenger whose age information was missing from the data set.
	The model was tested against a randomly reserved set of 20\% of the sample data.
	1,000 tests were completed yielding a mean RMSE of (14.11 $\pm$ 0.85) years.
	The RMSE of this model represents roughly half of a human generation.
	\begin{center}
	\includegraphics[width=0.45\textwidth]{agermsefreq}\\
	\end{center}
	The shape of the modeled age distribution is similar in center, shape, and spread to the known age distribution.
	Although the RMSE of the model is a significant portion of the human lifespan, there is no obvious reason to reject the results of the model.
	\begin{center}
	\includegraphics[width=0.45\textwidth]{agemodeldist}\\
	\end{center}
	The last feature which included missing data was Cabin.
	77\% of all passengers have no record of which cabin they stayed in.
	It was thought that no appreciable amount of information could be mined from this feature as it is.
	Therefore, the cabin feature did not recieve a modeling treatment.
	The feature was ultimately engineered into another feature "Deck."

\subsection{Feature Engineering}
	Within the data of the Kaggle dataset were some additional data.
	The data in the set was engineered to produce additional data for the model.
	Two features were engineered from the existing dataset: "Title" and "Deck."
	An appreciable amount of passengers did not have a known value for cabin.
	An effort was made to salvage any information in this feature by engineering the "Deck" feature.
	The "Deck" feature was created from the "Cabin" feature.
	Each known cabin number was prepended with a letter to indicate on which deck the cabin was located.
	This letter was stripped from all passengers whose cabin information was known and included in the new feature.
	Accordingly for all passengers whose cabin, and therefore deck, was unknown a question-mark acted as placeholder for the passenger's deck.
	\begin{center}
	\begin{tabular}{l r}
		Deck	& Count\\
		\hline
		? 	& 687\\
		A 	& 15\\
		B 	& 47\\
		C 	& 59\\
		D 	& 33\\
		E 	& 32\\
		F 	& 13\\
		G 	& 4\\
		T 	& 1\\
	\end{tabular}
	\end{center}


	Each passenger's name included a title like "Mrs" or "Capt."
	This title was stripped from the passenger and included in the dataset as another feature.\\
	\begin{center}
	\begin{tabular}{l r}
		Title         &	Freq.\\
		\hline
		Mr            & 517\\
		Miss          & 182\\
		Mrs           & 125\\
		Master         & 40\\
		Dr              & 7\\
		Rev             & 6\\
		Mlle            & 2\\
		Major           & 2\\
		Col             & 2\\
		Lady            & 1\\
		Capt            & 1\\
		Don             & 1\\
		Mme             & 1\\
		Sir             & 1\\
		theCountess     & 1\\
		Jonkheer        & 1\\
		Ms              & 1\\
	\end{tabular}
	\end{center}
\subsection{Feature Selection}
	
	The Kaggle data set includes features which were irrelevant or redundant to this study.
	The most striking example of irrelevance is PassengerId.
	The feature set of this study was established by calculating Pearson's r for correlation of each feature with survival.
	Features that were found to have a correlation of any strength at a significance level of 0.05 were included in the model feature set.

\section{Methodology}

\section{Results}

\section{Discussion}

\section{Conclusion}\label{conclusions}

\end{multicols}
\end{document}

%\refrences
%1 - https://arxiv.org/pdf/2006.11105.pdf
